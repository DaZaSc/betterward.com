---
layout: default
title: Chapter 1: The Gubrud Benchmark (1997) - Retrospective Benchmarks - Betterward
---
<style>
    /* Chapter-specific styles */
    .chapter-header {
        margin-bottom: 2em;
        text-align: center;
    }
    .chapter-header .series-title {
        font-family: 'IBM Plex Sans', sans-serif;
        font-size: 1em;
        color: var(--text-medium);
        margin-bottom: 0.5em;
        text-transform: uppercase;
        letter-spacing: 0.05em;
    }
    .chapter-header h1 {
        font-size: 2em;
        margin-bottom: 0.3em;
    }
    .chapter-header .subtitle {
        font-size: 1.1em;
        color: var(--text-medium);
        font-style: italic;
    }
    .chapter-header .author {
        margin-top: 1.5em;
        font-size: 1em;
    }
    .chapter-header .date {
        font-size: 0.9em;
        color: var(--text-light);
        margin-top: 0.3em;
    }
    .chapter-header .working-paper {
        font-size: 0.85em;
        color: var(--text-light);
        font-style: italic;
    }

    /* Score boxes */
    .score-box {
        background-color: #f9f9f9;
        border-left: 3px solid var(--primary-accent);
        padding: 1em 1.5em;
        margin: 1.5em 0;
        font-family: 'IBM Plex Sans', sans-serif;
        font-size: 0.95em;
    }
    .score-box .label {
        font-weight: 600;
        margin-bottom: 0.5em;
    }
    .score-box .option {
        margin: 0.3em 0;
    }
    .score-box .selected {
        font-weight: 600;
    }

    /* Tables */
    table {
        width: 100%;
        border-collapse: collapse;
        margin: 2em 0;
        font-size: 0.95em;
    }
    th, td {
        text-align: left;
        padding: 0.75em 1em;
        border-bottom: 1px solid var(--border);
    }
    th {
        font-family: 'IBM Plex Sans', sans-serif;
        font-weight: 600;
        background-color: #f9f9f9;
        border-bottom: 2px solid var(--primary-accent);
    }
    td:last-child, th:last-child {
        text-align: center;
    }
    tr.criterion-average td {
        font-weight: 600;
        background-color: #f9f9f9;
    }
    tr.overall-score td {
        font-weight: 700;
        font-size: 1.05em;
        border-top: 2px solid var(--primary-accent);
    }

    /* Endnotes */
    .endnotes {
        margin-top: 3em;
        padding-top: 2em;
        border-top: 2px solid var(--primary-accent);
    }
    .endnotes h2 {
        font-size: 1.3em;
        margin-bottom: 1em;
        border-top: none;
        padding-top: 0;
    }
    .endnotes ol {
        font-size: 0.9em;
        color: var(--text-medium);
        margin-left: 1.5em;
    }
    .endnotes li {
        margin-bottom: 0.8em;
        line-height: 1.6;
    }
    .endnotes a {
        word-break: break-all;
    }
    .backlink {
        font-size: 0.85em;
        margin-left: 0.3em;
    }

    /* Superscript references */
    sup.ref a {
        font-size: 0.75em;
        color: var(--wikipedia-blue);
        text-decoration: none;
        border: none;
    }
    sup.ref a:hover {
        text-decoration: underline;
    }

    /* PDF download link */
    .download-link {
        display: inline-block;
        margin: 1.5em 0;
        padding: 0.75em 1.5em;
        background-color: var(--primary-accent);
        color: white;
        font-family: 'IBM Plex Sans', sans-serif;
        font-size: 0.95em;
        border: none;
        transition: background-color 0.2s ease;
    }
    .download-link:hover {
        background-color: var(--hover-accent);
        color: white;
        border: none;
    }

    /* Checkmark styling */
    .check { color: #2eb85c; }
    .tilde { color: var(--text-medium); }
</style>

<article class="post-content">
    <div class="chapter-header">
        <p class="series-title"><a href="/retrospective-benchmarks/">Retrospective Benchmarks for Machine Intelligence</a></p>
        <p class="subtitle">Evaluating Current AI Against Historical Specifications</p>
        <h1>Chapter 1: The Gubrud Benchmark (1997)</h1>
        <p class="author">Dakota Schuck</p>
        <p class="date">December 2025</p>
        <p class="working-paper">Working paper. Comments welcome.</p>
    </div>

    <p><a href="gubrud-benchmark.pdf" class="download-link">Download PDF</a></p>

    <h2>Preface: Methodology</h2>

    <p>This document attempts something unusual: treating historical predictions and definitions of machine intelligence as testable specifications, then evaluating current AI systems against them.</p>

    <p>The approach is necessarily imperfect. We are:</p>
    <ul>
        <li>Applying 21st-century benchmarks to 20th-century (and earlier) concepts</li>
        <li>Asking whether systems meet specifications that weren't written as specifications</li>
        <li>Inviting the original thinkers to a conversation they cannot fully join</li>
    </ul>

    <p>We've tried to be rigorous where rigor is possible, explicit about uncertainty where it isn't, and honest about the gaps. Every claim should be cited; where citations are missing, we've marked them. Where we've made interpretive choices, we've flagged them.</p>

    <p>This is a first attempt.<sup class="ref"><a href="#fn1" id="ref1">[1]</a></sup> It is meant to be improved, corrected, and extended by others. If you can strengthen a citation, challenge an interpretation, or propose a better threshold—please do.</p>

    <h2>Introduction: The Term Worth Trillions</h2>

    <p>In the summer of 1997, a physics graduate student sat in a basement pump room at the University of Maryland, reading everything he could find about emerging technologies.<sup class="ref"><a href="#fn2" id="ref2">[2]</a></sup> Mark Gubrud was worried about autonomous weapons. That year, he submitted a paper to the Fifth Foresight Conference on Molecular Nanotechnology with a warning about how advanced AI could destabilize international security.<sup class="ref"><a href="#fn3" id="ref3">[3]</a></sup></p>

    <p>In that paper, he used a phrase no one had used before: <em>artificial general intelligence</em>.</p>

    <p>No one noticed. The term disappeared for nearly a decade.</p>

    <p>Around 2002, a group of AI researchers—including Shane Legg (later co-founder of DeepMind) and Ben Goertzel—were searching for a name for the kind of AI they wanted to build. They independently coined the same term.<sup class="ref"><a href="#fn4" id="ref4">[4]</a></sup> In 2005, Gubrud surfaced in an online forum to point out his priority. Legg's response, years later: "Someone comes out of nowhere and says, 'I invented the AGI definition in '97,' and we say, 'Who the hell are you?' Then we checked, and indeed there was a paper."<sup class="ref"><a href="#fn5" id="ref5">[5]</a></sup></p>

    <p>Today, "AGI" anchors contracts worth billions of dollars.<sup class="ref"><a href="#fn6" id="ref6">[6]</a></sup> The term Gubrud coined in a basement—while warning about the dangers of advanced AI—now names the explicit goal of the world's most valuable AI companies.</p>

    <p>Gubrud, now 67, lives in Colorado, caring for his mother.<sup class="ref"><a href="#fn7" id="ref7">[7]</a></sup> He has no steady job.<sup class="ref"><a href="#fn8" id="ref8">[8]</a></sup></p>

    <p>The question we're asking: If you could show Gubrud a current frontier AI system—say, Claude Opus 4.5—would he say, yes, this is what I meant?</p>

    <p>And not just Gubrud. What about Turing? Lovelace? McCarthy? Minsky? Each left us something like a specification. Did we meet it?</p>

    <h2>The Original Definition</h2>

    <p>From "Nanotechnology and International Security," presented at the Fifth Foresight Conference on Molecular Nanotechnology, November 1997:<sup class="ref"><a href="#fn9" id="ref9">[9]</a></sup></p>

    <blockquote>
        <p>. . . artificial general intelligence. . . AI systems that rival or surpass the human brain in complexity and speed, that can acquire, manipulate and reason with general knowledge, and that are usable in essentially any phase of industrial or military operations where a human intelligence would otherwise be needed.</p>
    </blockquote>

    <h3>Context</h3>

    <p>Gubrud wasn't writing an AI paper. He was writing a security paper. "AGI" appeared alongside nanotechnology and other emerging technologies as potential destabilizers of international order.<sup class="ref"><a href="#fn10" id="ref10">[10]</a></sup> His concern was weaponization and arms races, not capability benchmarks.</p>

    <p>This matters for interpretation: Gubrud's "general intelligence" was meant to contrast with narrow, task-specific systems. His reference to "industrial or military operations" wasn't arbitrary—it reflected his focus on domains where autonomous systems could substitute for human judgment in consequential decisions.</p>

    <h3>Operationalization</h3>

    <p>We extract six criteria from Gubrud's definition, in his order:</p>
    <ol>
        <li>Rival or surpass human brain in complexity</li>
        <li>Rival or surpass human brain in speed</li>
        <li>Acquire general knowledge</li>
        <li>Manipulate general knowledge</li>
        <li>Reason with general knowledge</li>
        <li>Usable where human intelligence would otherwise be needed</li>
    </ol>

    <p>For each criterion, we identify subcriteria, existing measures where available, thresholds, and an assessment.</p>

    <p>Scoring:</p>
    <ul>
        <li>☐ 0% — Clearly does not meet criterion</li>
        <li>☐ 50% — Contested; reasonable arguments exist on both sides</li>
        <li>☐ 100% — Clearly meets criterion</li>
    </ul>

    <h2>Criterion 1: Rival or Surpass Human Brain in Complexity</h2>

    <h3>What Gubrud Probably Meant</h3>

    <p>In 1997, "complexity" in the context of brains likely referred to the scale and interconnection of neural structures. The comparison to the human brain suggests Gubrud imagined systems approaching biological scale—not necessarily identical architecture, but comparable information-processing capacity.</p>

    <h3>Structural Scale</h3>

    <p><strong>Measure:</strong> Model parameter count vs. estimates of human brain synaptic connections</p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Human brain: ~86 billion neurons, ~100–600 trillion synapses<sup class="ref"><a href="#fn11" id="ref11">[11]</a></sup></li>
        <li>Human language-specific regions (Broca's and Wernicke's areas): ~400–700 billion effective parameters by one estimate<sup class="ref"><a href="#fn12" id="ref12">[12]</a></sup></li>
        <li>Frontier LLMs (2025): ~1–2 trillion parameters<sup class="ref"><a href="#fn13" id="ref13">[13]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≥100 trillion parameters (full-brain parity) OR ≥500 billion (language-region parity)</p>

    <p><strong>Assessment:</strong> Current models are within an order of magnitude of language-specific brain regions but remain 100–600× below full-brain synapse counts.</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <p><strong>Caveats:</strong> Parameter-synapse comparisons are architecturally problematic.<sup class="ref"><a href="#fn14" id="ref14">[14]</a></sup> Synapses have dynamic, continuous-valued states; parameters are fixed post-training. A bee brain has ~1 million neurons and performs complex navigation.<sup class="ref"><a href="#fn15" id="ref15">[15]</a></sup> Scale may not be the right measure of complexity.</p>

    <h3>Functional Complexity (Task Diversity)</h3>

    <p><strong>Measure:</strong> Number of distinct cognitive task categories performed at human-competent level</p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>MMLU benchmark: 57 subject areas<sup class="ref"><a href="#fn16" id="ref16">[16]</a></sup></li>
        <li>BIG-Bench: 204 tasks<sup class="ref"><a href="#fn17" id="ref17">[17]</a></sup></li>
        <li>Human competence: Thousands of task types<sup class="ref"><a href="#fn18" id="ref18">[18]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> Competent performance (≥50th percentile among humans) across ≥100 cognitively distinct task categories</p>

    <p><strong>Current performance:</strong></p>
    <ul>
        <li>Frontier LLMs score ≥85% on MMLU, covering 57 subjects<sup class="ref"><a href="#fn19" id="ref19">[19]</a></sup></li>
        <li>Performance across BIG-Bench tasks is variable but broadly competent<sup class="ref"><a href="#fn20" id="ref20">[20]</a></sup></li>
    </ul>

    <p><strong>Assessment:</strong> Frontier models demonstrate breadth across dozens to hundreds of task categories. Whether this constitutes complexity rivaling the human brain depends on interpretation.</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option">☐ 50% — Contested</div>
        <div class="option selected">☒ 100% — Clearly meets criterion</div>
    </div>

    <h3>Architectural Sophistication</h3>

    <p><strong>Measure:</strong> Presence of dynamic, adaptive features beyond static feedforward networks</p>

    <p><strong>Reference values for human brain:</strong> Persistent memory, real-time learning, attention modulation, self-monitoring, multi-modal integration<sup class="ref"><a href="#fn21" id="ref21">[21]</a></sup></p>

    <p><strong>Feature assessment:</strong></p>
    <ul>
        <li>Persistent memory across sessions — Limited; depends on deployment<sup class="ref"><a href="#fn22" id="ref22">[22]</a></sup></li>
        <li>In-context learning — Present<sup class="ref"><a href="#fn23" id="ref23">[23]</a></sup></li>
        <li>Tool use — Present<sup class="ref"><a href="#fn24" id="ref24">[24]</a></sup></li>
        <li>Multi-modal integration — Present<sup class="ref"><a href="#fn25" id="ref25">[25]</a></sup></li>
        <li>True self-modification/online learning — Absent during inference<sup class="ref"><a href="#fn26" id="ref26">[26]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≥4 of 5 features with human-like flexibility</p>

    <p><strong>Assessment:</strong> 3 of 5 features present; persistent memory and self-modification remain limited.</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h2>Criterion 2: Rival or Surpass Human Brain in Speed</h2>

    <h3>What Gubrud Probably Meant</h3>

    <p>Processing speed—how quickly the system can take in information and produce outputs. In 1997, human cognition was clearly faster than existing AI for most tasks.</p>

    <h3>Text Generation Speed</h3>

    <p><strong>Measure:</strong> Output tokens per second vs. human speaking/writing speed</p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Human speaking: ~125–150 words/minute ≈ 2–3 words/second ≈ 3–4 tokens/second<sup class="ref"><a href="#fn27" id="ref27">[27]</a></sup></li>
        <li>Human typing (average): ~40 words/minute ≈ ~1 token/second<sup class="ref"><a href="#fn28" id="ref28">[28]</a></sup></li>
        <li>Frontier LLMs: 50–200 tokens/second typical; up to 1,800 tokens/second on specialized hardware<sup class="ref"><a href="#fn29" id="ref29">[29]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≥10× human speaking speed (≥30 tokens/second)</p>

    <p><strong>Current performance:</strong> Frontier models exceed 50 tokens/second routinely.<sup class="ref"><a href="#fn30" id="ref30">[30]</a></sup></p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option">☐ 50% — Contested</div>
        <div class="option selected">☒ 100% — Clearly meets criterion</div>
    </div>

    <h3>Text Processing Speed</h3>

    <p><strong>Measure:</strong> Input tokens processed per second vs. human reading speed</p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Human reading: ~200–300 words/minute ≈ 4–5 tokens/second<sup class="ref"><a href="#fn31" id="ref31">[31]</a></sup></li>
        <li>LLM prompt processing: Thousands of tokens/second<sup class="ref"><a href="#fn32" id="ref32">[32]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≥100× human reading speed (≥500 tokens/second)</p>

    <p><strong>Current performance:</strong> Exceeds threshold by large margin.</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option">☐ 50% — Contested</div>
        <div class="option selected">☒ 100% — Clearly meets criterion</div>
    </div>

    <h3>Response Latency</h3>

    <p><strong>Measure:</strong> Time to first token (TTFT)</p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Human conversational response: ~200–500ms for simple replies<sup class="ref"><a href="#fn33" id="ref33">[33]</a></sup></li>
        <li>Frontier LLMs: ~100–500ms typical TTFT<sup class="ref"><a href="#fn34" id="ref34">[34]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≤500ms for standard queries</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option">☐ 50% — Contested</div>
        <div class="option selected">☒ 100% — Clearly meets criterion</div>
    </div>

    <h3>Reasoning Speed</h3>

    <p><strong>Measure:</strong> Time to solve complex problems vs. human experts at equivalent accuracy</p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Human expert on GPQA-level problem: Minutes to tens of minutes<sup class="ref"><a href="#fn35" id="ref35">[35]</a></sup></li>
        <li>LLMs with extended thinking: Seconds to minutes<sup class="ref"><a href="#fn36" id="ref36">[36]</a></sup></li>
    </ul>

    <p><strong>Assessment:</strong> For problems current AI can solve, speed is comparable or faster. For problems requiring extended deliberation, timing varies.</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h2>Criterion 3: Acquire General Knowledge</h2>

    <h3>What Gubrud Probably Meant</h3>

    <p>The ability to gain knowledge—to learn. In 1997, machine learning existed but was narrow. "Acquire general knowledge" implies learning across domains, not just pattern-matching on fixed training data.</p>

    <h3>Few-Shot Learning Efficiency</h3>

    <p><strong>Measure:</strong> Performance improvement per example on novel tasks</p>

    <p><strong>Benchmark:</strong> ARC-AGI (Abstraction and Reasoning Corpus), explicitly designed to test skill acquisition efficiency<sup class="ref"><a href="#fn37" id="ref37">[37]</a></sup></p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Humans: ~73–77% on ARC-AGI-1 public tasks<sup class="ref"><a href="#fn38" id="ref38">[38]</a></sup></li>
        <li>Best AI (late 2024): ~55% on ARC-AGI-1 private set<sup class="ref"><a href="#fn39" id="ref39">[39]</a></sup></li>
        <li>OpenAI o3 (Dec 2024): ~87.5% on ARC-AGI-1 (high compute)<sup class="ref"><a href="#fn40" id="ref40">[40]</a></sup></li>
        <li>AI on ARC-AGI-2 (2025): Single-digit percentages<sup class="ref"><a href="#fn41" id="ref41">[41]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≥85% on ARC-AGI-1 (the competition target)</p>

    <p><strong>Assessment:</strong> o3 crossed the 85% threshold on ARC-AGI-1, but ARC-AGI-2 remains largely unsolved. The ability to acquire genuinely novel skills efficiently remains contested.</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h3>Knowledge Breadth</h3>

    <p><strong>Measure:</strong> Factual knowledge across domains</p>

    <p><strong>Benchmark:</strong> MMLU (Massive Multitask Language Understanding)—57 subjects from elementary to professional level<sup class="ref"><a href="#fn42" id="ref42">[42]</a></sup></p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Human expert ceiling: ~90% (estimated)<sup class="ref"><a href="#fn43" id="ref43">[43]</a></sup></li>
        <li>Claude Opus 4.5: ~88–90%<sup class="ref"><a href="#fn44" id="ref44">[44]</a></sup></li>
        <li>Frontier models generally: 88–91%<sup class="ref"><a href="#fn45" id="ref45">[45]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≥85% MMLU accuracy</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option">☐ 50% — Contested</div>
        <div class="option selected">☒ 100% — Clearly meets criterion</div>
    </div>

    <p><strong>Caveat:</strong> MMLU is now considered near-saturated and may not distinguish frontier models.<sup class="ref"><a href="#fn46" id="ref46">[46]</a></sup></p>

    <h3>Real-Time Knowledge Acquisition (Tool Use)</h3>

    <p><strong>Measure:</strong> Ability to retrieve and integrate new information during task execution</p>

    <p><strong>Capabilities present:</strong> Web search, document retrieval, API access<sup class="ref"><a href="#fn47" id="ref47">[47]</a></sup></p>

    <p><strong>Assessment:</strong> Tool use exists but integration is imperfect; hallucination and retrieval failures occur.<sup class="ref"><a href="#fn48" id="ref48">[48]</a></sup></p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h2>Criterion 4: Manipulate General Knowledge</h2>

    <h3>What Gubrud Probably Meant</h3>

    <p>Not just storing knowledge but working with it—transforming, combining, applying it flexibly across contexts.</p>

    <h3>Cross-Domain Transfer</h3>

    <p><strong>Measure:</strong> Application of knowledge from one domain to problems in another</p>

    <p><strong>Existing benchmarks:</strong> Limited standardization<sup class="ref"><a href="#fn49" id="ref49">[49]</a></sup></p>

    <p><strong>Assessment:</strong> LLMs demonstrate some analogical transfer<sup class="ref"><a href="#fn50" id="ref50">[50]</a></sup> but also exhibit surprising failures when surface features change.<sup class="ref"><a href="#fn51" id="ref51">[51]</a></sup></p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h3>Knowledge Synthesis</h3>

    <p><strong>Measure:</strong> Combining multiple sources into coherent novel outputs</p>

    <p><strong>Assessment:</strong> LLMs can synthesize information within context windows but quality varies; long-document synthesis remains challenging.<sup class="ref"><a href="#fn52" id="ref52">[52]</a></sup></p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h3>Belief Revision</h3>

    <p><strong>Measure:</strong> Updating conclusions when given contradictory evidence</p>

    <p><strong>Assessment:</strong> Within-context updating is possible but inconsistent; models can struggle to override strong training priors.<sup class="ref"><a href="#fn53" id="ref53">[53]</a></sup></p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h2>Criterion 5: Reason with General Knowledge</h2>

    <h3>What Gubrud Probably Meant</h3>

    <p>Drawing inferences, solving problems, reaching conclusions—the core of "intelligence" in most definitions.</p>

    <h3>Expert-Level Reasoning</h3>

    <p><strong>Benchmark:</strong> GPQA-Diamond (graduate-level science questions designed to be difficult even for PhDs)<sup class="ref"><a href="#fn54" id="ref54">[54]</a></sup></p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Human PhD experts: ~65% accuracy<sup class="ref"><a href="#fn55" id="ref55">[55]</a></sup></li>
        <li>Claude Opus 4.5: ~87%<sup class="ref"><a href="#fn56" id="ref56">[56]</a></sup></li>
        <li>Gemini 3 Pro: ~92%<sup class="ref"><a href="#fn57" id="ref57">[57]</a></sup></li>
        <li>GPT-5.1: ~88%<sup class="ref"><a href="#fn58" id="ref58">[58]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≥65% (human expert level)</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option">☐ 50% — Contested</div>
        <div class="option selected">☒ 100% — Clearly meets criterion</div>
    </div>

    <h3>Mathematical Reasoning</h3>

    <p><strong>Benchmarks:</strong> MATH, AIME (American Invitational Mathematics Examination)</p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Top 500 US high school students: ~90% AIME<sup class="ref"><a href="#fn59" id="ref59">[59]</a></sup></li>
        <li>OpenAI o3: 96.7% AIME<sup class="ref"><a href="#fn60" id="ref60">[60]</a></sup></li>
        <li>Other frontier models: Variable; many below 90% threshold<sup class="ref"><a href="#fn61" id="ref61">[61]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> Top-500 national performance (≥90% AIME)</p>

    <p><strong>Assessment:</strong> Some models (o3) exceed threshold; others (Claude) do not.</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h3>Abstract Reasoning on Novel Problems</h3>

    <p><strong>Benchmark:</strong> ARC-AGI</p>

    <p>(See Section 5.1 above)</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h3>Causal and Counterfactual Reasoning</h3>

    <p><strong>Existing benchmarks:</strong> Limited standardization<sup class="ref"><a href="#fn62" id="ref62">[62]</a></sup></p>

    <p><strong>Assessment:</strong> LLMs show some causal reasoning capability but struggle with complex counterfactuals.<sup class="ref"><a href="#fn63" id="ref63">[63]</a></sup></p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h2>Criterion 6: Usable Where Human Intelligence Would Otherwise Be Needed</h2>

    <h3>What Gubrud Probably Meant</h3>

    <p>Gubrud specified "essentially any phase of industrial or military operations." This is an application criterion, not a capability criterion. He was asking: can this substitute for humans in real-world consequential tasks?</p>

    <h3>Autonomous Task Completion</h3>

    <p><strong>Benchmark:</strong> SWE-Bench Verified (real GitHub issues requiring code changes)<sup class="ref"><a href="#fn64" id="ref64">[64]</a></sup></p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Claude Opus 4.5: ~81%<sup class="ref"><a href="#fn65" id="ref65">[65]</a></sup></li>
        <li>GPT-5.1: ~72%<sup class="ref"><a href="#fn66" id="ref66">[66]</a></sup></li>
        <li>Gemini 3 Pro: ~77%<sup class="ref"><a href="#fn67" id="ref67">[67]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≥70% on SWE-Bench Verified</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option">☐ 50% — Contested</div>
        <div class="option selected">☒ 100% — Clearly meets criterion</div>
    </div>

    <h3>Deployment Reliability</h3>

    <p><strong>Measure:</strong> Error rates in production, particularly hallucination</p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Hallucination rates: Highly variable by task, domain, and model; no consensus benchmark exists<sup class="ref"><a href="#fn68" id="ref68">[68]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> ≤10% critical error rate</p>

    <p><strong>Assessment:</strong> Hallucination remains a significant concern in deployed systems.</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h3>Domain Coverage</h3>

    <p><strong>Measure:</strong> Breadth of applicable domains per Gubrud's "essentially any phase"</p>

    <p><strong>Assessment:</strong> Strong in knowledge work (writing, analysis, coding); limited in physical operations, real-time control, and embodied tasks.<sup class="ref"><a href="#fn69" id="ref69">[69]</a></sup></p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h3>Economic Substitution</h3>

    <p><strong>Measure:</strong> Demonstrated ability to substitute for human labor in professional categories</p>

    <p><strong>Reference values:</strong></p>
    <ul>
        <li>Productivity gains from AI assistance: Significant gains documented in specific tasks; Noy & Zhang (2023) found ~40% productivity increase for writing tasks among mid-skill workers<sup class="ref"><a href="#fn70" id="ref70">[70]</a></sup></li>
        <li>Full task substitution: Limited to narrow domains<sup class="ref"><a href="#fn71" id="ref71">[71]</a></sup></li>
    </ul>

    <p><strong>Threshold:</strong> Demonstrated substitution OR substantial productivity enhancement in ≥3 professional categories</p>

    <div class="score-box">
        <div class="label">Score:</div>
        <div class="option">☐ 0% — Clearly does not meet criterion</div>
        <div class="option selected">☒ 50% — Contested</div>
        <div class="option">☐ 100% — Clearly meets criterion</div>
    </div>

    <h2>Summary: The Gubrud Benchmark</h2>

    <table>
        <thead>
            <tr>
                <th>Criterion</th>
                <th>Subcriterion</th>
                <th>Score</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td rowspan="4">1. Complexity</td>
                <td>1.1 Structural scale</td>
                <td>50%</td>
            </tr>
            <tr>
                <td>1.2 Functional complexity</td>
                <td>100%</td>
            </tr>
            <tr>
                <td>1.3 Architectural sophistication</td>
                <td>50%</td>
            </tr>
            <tr class="criterion-average">
                <td><strong>Criterion average</strong></td>
                <td><strong>67%</strong></td>
            </tr>
            <tr>
                <td rowspan="5">2. Speed</td>
                <td>2.1 Text generation</td>
                <td>100%</td>
            </tr>
            <tr>
                <td>2.2 Text processing</td>
                <td>100%</td>
            </tr>
            <tr>
                <td>2.3 Response latency</td>
                <td>100%</td>
            </tr>
            <tr>
                <td>2.4 Reasoning speed</td>
                <td>50%</td>
            </tr>
            <tr class="criterion-average">
                <td><strong>Criterion average</strong></td>
                <td><strong>88%</strong></td>
            </tr>
            <tr>
                <td rowspan="4">3. Acquire knowledge</td>
                <td>3.1 Few-shot learning</td>
                <td>50%</td>
            </tr>
            <tr>
                <td>3.2 Knowledge breadth</td>
                <td>100%</td>
            </tr>
            <tr>
                <td>3.3 Real-time acquisition</td>
                <td>50%</td>
            </tr>
            <tr class="criterion-average">
                <td><strong>Criterion average</strong></td>
                <td><strong>67%</strong></td>
            </tr>
            <tr>
                <td rowspan="4">4. Manipulate knowledge</td>
                <td>4.1 Cross-domain transfer</td>
                <td>50%</td>
            </tr>
            <tr>
                <td>4.2 Knowledge synthesis</td>
                <td>50%</td>
            </tr>
            <tr>
                <td>4.3 Belief revision</td>
                <td>50%</td>
            </tr>
            <tr class="criterion-average">
                <td><strong>Criterion average</strong></td>
                <td><strong>50%</strong></td>
            </tr>
            <tr>
                <td rowspan="5">5. Reason with knowledge</td>
                <td>5.1 Expert reasoning</td>
                <td>100%</td>
            </tr>
            <tr>
                <td>5.2 Mathematical reasoning</td>
                <td>50%</td>
            </tr>
            <tr>
                <td>5.3 Abstract reasoning</td>
                <td>50%</td>
            </tr>
            <tr>
                <td>5.4 Causal reasoning</td>
                <td>50%</td>
            </tr>
            <tr class="criterion-average">
                <td><strong>Criterion average</strong></td>
                <td><strong>63%</strong></td>
            </tr>
            <tr>
                <td rowspan="5">6. Usable where needed</td>
                <td>6.1 Task completion</td>
                <td>100%</td>
            </tr>
            <tr>
                <td>6.2 Reliability</td>
                <td>50%</td>
            </tr>
            <tr>
                <td>6.3 Domain coverage</td>
                <td>50%</td>
            </tr>
            <tr>
                <td>6.4 Economic substitution</td>
                <td>50%</td>
            </tr>
            <tr class="criterion-average">
                <td><strong>Criterion average</strong></td>
                <td><strong>63%</strong></td>
            </tr>
            <tr class="overall-score">
                <td colspan="2"><strong>Overall Gubrud Benchmark Score</strong></td>
                <td><strong>66%</strong></td>
            </tr>
        </tbody>
    </table>

    <h2>Interpretation</h2>

    <h3>What Frontier AI Clearly Achieves (100%)</h3>
    <ul>
        <li>Speed in text generation and processing</li>
        <li>Breadth of factual knowledge</li>
        <li>Expert-level reasoning on structured problems</li>
        <li>Specific task completion (e.g., software engineering)</li>
    </ul>

    <h3>What Remains Contested (50%)</h3>
    <ul>
        <li>Structural complexity parity</li>
        <li>Novel skill acquisition</li>
        <li>Knowledge manipulation and transfer</li>
        <li>Abstract and causal reasoning</li>
        <li>Deployment reliability</li>
        <li>Broad domain applicability</li>
    </ul>

    <h3>What Is Clearly Not Achieved (0%)</h3>

    <p>None of the subcriteria score 0% for frontier models—but several 50% scores reflect generous interpretation of ambiguous evidence.</p>

    <h2>The Verdict (Provisional)</h2>

    <p>Gubrud's 1997 definition describes a system that:</p>
    <ul>
        <li>Matches brain speed <span class="check">✓</span> (clearly exceeded)</li>
        <li>Matches brain complexity <span class="tilde">~</span> (approached for specific functions, not full-brain)</li>
        <li>Can acquire general knowledge <span class="tilde">~</span> (broad but not human-flexible)</li>
        <li>Can manipulate general knowledge <span class="tilde">~</span> (present but inconsistent)</li>
        <li>Can reason with general knowledge <span class="tilde">~</span> (strong on formal, weaker on novel)</li>
        <li>Is usable in essentially any operation <span class="tilde">~</span> (many cognitive tasks, not physical/real-time)</li>
    </ul>

    <p>At 66%, <strong>current frontier AI sits at the boundary</strong>. A reasonable case can be made that Gubrud's definition is substantially met; an equally reasonable case can be made that the generality implicit in "general knowledge" and "essentially any phase" has not been achieved.</p>

    <p>We do not attempt to speak for Gubrud. He is alive and can speak for himself.<sup class="ref"><a href="#fn72" id="ref72">[72]</a></sup></p>

    <h2>Methodological Notes</h2>

    <p>This evaluation uses an intentionally coarse scoring system (0%/50%/100%) and unweighted criteria. This is a deliberate choice.</p>

    <p>Finer gradations would imply precision we do not have. A score of 65% versus 70% would suggest a confidence in measurement that no current benchmark supports. The three-point scale forces honesty: either the evidence clearly supports a claim, clearly refutes it, or the matter is genuinely contested.</p>

    <p>Differential weighting would require judgments about Gubrud's priorities that we cannot make with confidence. Did he consider "speed" more or less important than "general knowledge"? His 1997 text does not say. We could guess, but we would rather be honestly approximate than precisely wrong.</p>

    <p>The subcriteria themselves reflect operationalization choices that are contestable. Why measure complexity via parameter count rather than algorithmic depth? Why use ARC-AGI rather than another skill-acquisition benchmark? These choices are defensible but not uniquely correct. Different operationalizations might yield different scores.</p>

    <p>The goal is accuracy at the expense of precision. Readers who disagree with specific operationalizations, who believe certain criteria should be weighted more heavily, or who have better data for any assessment are invited to propose alternatives. The appendix in the PDF version provides a blank scorecard for exactly this purpose.</p>

    <h2>Citation Gaps and Requests for Collaboration</h2>

    <p>The following claims would benefit from stronger sourcing:</p>
    <ul>
        <li>Exact parameter counts for Claude Opus 4.5, GPT-5, Gemini 3 Pro</li>
        <li>Timed human expert performance on GPQA</li>
        <li>Systematic taxonomy of human cognitive task categories</li>
        <li>Systematic count of BIG-Bench tasks at ≥50th percentile human performance</li>
        <li>Rigorous human baseline on full MMLU</li>
        <li>Systematic error rates for retrieval-augmented generation</li>
        <li>Systematic transfer learning benchmarks for LLMs</li>
        <li>Systematic benchmarks for multi-source synthesis</li>
        <li>Systematic belief revision benchmarks</li>
        <li>Official AIME scores for frontier models other than o3</li>
        <li>Systematic review of LLM causal reasoning capabilities</li>
        <li>Systematic meta-analysis of hallucination rates across tasks and models</li>
        <li>Systematic meta-analysis of AI productivity effects across domains</li>
    </ul>

    <p>If you can fill any of these gaps, please contribute.</p>

    <!-- Endnotes -->
    <div class="endnotes">
        <h2>Notes</h2>
        <ol>
            <li id="fn1">AI Assistance Disclosure: Research, drafting, and analysis were conducted with the assistance of Claude Opus 4.5 (Anthropic, 2025). The AI contributed literature review, benchmark operationalization, and self-assessment of AI capabilities. The author provided editorial direction, methodological framing, and final approval. Responsibility for all claims rests with the author. <a href="#ref1" class="backlink">↩</a></li>
            <li id="fn2">"He spent all day buried in the noisy pump room on the basement floor of the laboratory, sitting there reading everything he could find." 36kr.com, "He Invented Trillion-Worth AGI but Now Is Down and Out," 2025. <a href="https://36kr.com/p/2689463822082945">https://36kr.com/p/2689463822082945</a> <a href="#ref2" class="backlink">↩</a></li>
            <li id="fn3">Gubrud, Mark A. "Nanotechnology and International Security." Fifth Foresight Conference on Molecular Nanotechnology, November 1997. <a href="https://legacy.foresight.org/Conferences/MNT05/Papers/Gubrud/index.html">https://legacy.foresight.org/Conferences/MNT05/Papers/Gubrud/index.html</a> <a href="#ref3" class="backlink">↩</a></li>
            <li id="fn4">Legg, Shane. Quoted in various interviews; see also Goertzel, Ben, ed. <em>Artificial General Intelligence</em>. Springer, 2007. <a href="#ref4" class="backlink">↩</a></li>
            <li id="fn5">36kr.com, op. cit. <a href="#ref5" class="backlink">↩</a></li>
            <li id="fn6">OpenAI's partnership with Microsoft reportedly values AGI-related IP in the hundreds of billions. Specific contract terms are not public. <a href="#ref6" class="backlink">↩</a></li>
            <li id="fn7">36kr.com, op. cit. Article dated 2025 states Gubrud is 67. <a href="#ref7" class="backlink">↩</a></li>
            <li id="fn8">Ibid. <a href="#ref8" class="backlink">↩</a></li>
            <li id="fn9">Gubrud 1997, op. cit. Full quote also cited in: Morris, Meredith Ringel, et al. "Levels of AGI for Operationalizing Progress on the Path to AGI." arXiv:2311.02462, 2023. <a href="https://arxiv.org/abs/2311.02462">https://arxiv.org/abs/2311.02462</a>; METR, "AGI: Definitions and Potential Impacts," 2024. <a href="#ref9" class="backlink">↩</a></li>
            <li id="fn10">Gubrud's paper focused primarily on nanotechnology and international security; AGI appears as one of several destabilizing emerging technologies. <a href="#ref10" class="backlink">↩</a></li>
            <li id="fn11">Azevedo, Frederico A.C., et al. "Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain." <em>Journal of Comparative Neurology</em> 513.5 (2009): 532–541. <a href="https://doi.org/10.1002/cne.21974">https://doi.org/10.1002/cne.21974</a> <a href="#ref11" class="backlink">↩</a></li>
            <li id="fn12">Millidge, Beren. "The Scale of the Brain vs Machine Learning." beren.io, 2022. <a href="https://www.beren.io/2022-01-30-The-Scale-of-the-Brain-vs-Machine-Learning/">https://www.beren.io/2022-01-30-The-Scale-of-the-Brain-vs-Machine-Learning/</a> <a href="#ref12" class="backlink">↩</a></li>
            <li id="fn13">Model parameter counts for frontier systems are not always publicly disclosed. GPT-4 was reported at ~1.8T parameters (unconfirmed); Claude and Gemini parameter counts are not public. Specific parameter counts for Claude Opus 4.5, GPT-5, and Gemini 3 Pro would strengthen this estimate. <a href="#ref13" class="backlink">↩</a></li>
            <li id="fn14">See discussion in Millidge 2022, op. cit., and Crawford, Hal. "AI versus the human brain." halcrawford.substack.com, 2024. <a href="https://halcrawford.substack.com/p/ai-versus-the-human-brain">https://halcrawford.substack.com/p/ai-versus-the-human-brain</a> <a href="#ref14" class="backlink">↩</a></li>
            <li id="fn15">Menzel, Randolf, and Martin Giurfa. "Cognitive architecture of a mini-brain: the honeybee." <em>Trends in Cognitive Sciences</em> 5.2 (2001): 62–71. <a href="https://doi.org/10.1016/S1364-6613(00)01601-6">https://doi.org/10.1016/S1364-6613(00)01601-6</a> <a href="#ref15" class="backlink">↩</a></li>
            <li id="fn16">Hendrycks, Dan, et al. "Measuring Massive Multitask Language Understanding." arXiv:2009.03300, 2020. <a href="https://arxiv.org/abs/2009.03300">https://arxiv.org/abs/2009.03300</a> <a href="#ref16" class="backlink">↩</a></li>
            <li id="fn17">Srivastava, Aarohi, et al. "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models." arXiv:2206.04615, 2022. <a href="https://arxiv.org/abs/2206.04615">https://arxiv.org/abs/2206.04615</a> <a href="#ref17" class="backlink">↩</a></li>
            <li id="fn18">Estimate based on breadth of human cognitive abilities. A systematic taxonomy of human cognitive task categories would provide a more rigorous comparison. <a href="#ref18" class="backlink">↩</a></li>
            <li id="fn19">Various benchmark reports; see Artificial Analysis, "Claude Opus 4.5 Benchmarks," November 2025. <a href="https://artificialanalysis.ai/">https://artificialanalysis.ai/</a> <a href="#ref19" class="backlink">↩</a></li>
            <li id="fn20">A systematic count of tasks at ≥50th percentile human performance would strengthen this assessment. <a href="#ref20" class="backlink">↩</a></li>
            <li id="fn21">Standard neuroscience; see e.g., Kandel, Eric R., et al. <em>Principles of Neural Science</em>. 5th ed., McGraw-Hill, 2013. <a href="https://neurology.mhmedical.com/book.aspx?bookID=1049">https://neurology.mhmedical.com/book.aspx?bookID=1049</a> <a href="#ref21" class="backlink">↩</a></li>
            <li id="fn22">Memory features vary by deployment. Claude.ai offers memory features; API deployments typically do not persist state. <a href="#ref22" class="backlink">↩</a></li>
            <li id="fn23">Brown, Tom, et al. "Language Models are Few-Shot Learners." NeurIPS 2020. <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a> <a href="#ref23" class="backlink">↩</a></li>
            <li id="fn24">Schick, Timo, et al. "Toolformer: Language Models Can Teach Themselves to Use Tools." arXiv:2302.04761, 2023. <a href="https://arxiv.org/abs/2302.04761">https://arxiv.org/abs/2302.04761</a> <a href="#ref24" class="backlink">↩</a></li>
            <li id="fn25">Multimodal models including GPT-4V, Gemini, Claude 3+ support image, audio, and in some cases video input. <a href="#ref25" class="backlink">↩</a></li>
            <li id="fn26">Current LLMs do not update weights during inference. Fine-tuning requires separate training runs. <a href="#ref26" class="backlink">↩</a></li>
            <li id="fn27">Typically cited speaking rate. See: Yuan, Jiahong, et al. "Towards an integrated understanding of speaking rate in conversation." INTERSPEECH 2006. <a href="https://www.isca-archive.org/interspeech_2006/yuan06_interspeech.html">https://www.isca-archive.org/interspeech_2006/yuan06_interspeech.html</a> <a href="#ref27" class="backlink">↩</a></li>
            <li id="fn28">Typing speed varies widely. 40 WPM is often cited as average. See various typing studies. <a href="#ref28" class="backlink">↩</a></li>
            <li id="fn29">Cerebras. "Introducing Cerebras Inference: AI at Instant Speed." cerebras.ai, 2024. <a href="https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed">https://cerebras.ai/blog/introducing-cerebras-inference-ai-at-instant-speed</a> <a href="#ref29" class="backlink">↩</a></li>
            <li id="fn30">Artificial Analysis, op. cit., and various model benchmarks. <a href="#ref30" class="backlink">↩</a></li>
            <li id="fn31">Reading speed varies. 200–300 WPM is commonly cited for adult reading. See: Rayner, Keith, et al. "Eye movements and information processing during reading." <em>Psychological Bulletin</em> 124.3 (1998): 372–422. <a href="https://doi.org/10.1037/0033-2909.124.3.372">https://doi.org/10.1037/0033-2909.124.3.372</a> <a href="#ref31" class="backlink">↩</a></li>
            <li id="fn32">Prompt processing speed varies by model and hardware; generally measured in thousands of tokens/second. <a href="#ref32" class="backlink">↩</a></li>
            <li id="fn33">Human response latency in conversation is typically 200–500ms for turn-taking. See: Stivers, Tanya, et al. "Universals and cultural variation in turn-taking in conversation." <em>PNAS</em> 106.26 (2009): 10587–10592. <a href="https://doi.org/10.1073/pnas.0903616106">https://doi.org/10.1073/pnas.0903616106</a> <a href="#ref33" class="backlink">↩</a></li>
            <li id="fn34">Various model benchmarks report TTFT in the 100–500ms range for standard queries. <a href="#ref34" class="backlink">↩</a></li>
            <li id="fn35">Estimate based on problem complexity. Timed human expert performance data on GPQA would provide a more rigorous baseline. <a href="#ref35" class="backlink">↩</a></li>
            <li id="fn36">Extended thinking / reasoning models (o1, Claude thinking mode) can take seconds to minutes depending on problem complexity. <a href="#ref36" class="backlink">↩</a></li>
            <li id="fn37">Chollet, François. "On the Measure of Intelligence." arXiv:1911.01547, 2019. <a href="https://arxiv.org/abs/1911.01547">https://arxiv.org/abs/1911.01547</a> <a href="#ref37" class="backlink">↩</a></li>
            <li id="fn38">Johnson, Aaditya, et al. "Testing ARC on Humans: A Large-Scale Assessment." NYU, 2024. Reported 73.3–77.2% average accuracy. <a href="https://lab42.global/arc-agi-benchmark-human-study/">https://lab42.global/arc-agi-benchmark-human-study/</a> <a href="#ref38" class="backlink">↩</a></li>
            <li id="fn39">ARC Prize 2024 Technical Report. arcprize.org, December 2024. <a href="https://arcprize.org/">https://arcprize.org/</a> <a href="#ref39" class="backlink">↩</a></li>
            <li id="fn40">OpenAI. "Introducing o3." December 2024. <a href="https://openai.com/index/deliberative-alignment/">https://openai.com/index/deliberative-alignment/</a>; François Chollet, social media announcements, December 2024. <a href="#ref40" class="backlink">↩</a></li>
            <li id="fn41">ARC-AGI-2 was released in early 2025; as of late 2025, top scores remain in single-digit percentages. See <a href="https://arcprize.org/">https://arcprize.org/</a> <a href="#ref41" class="backlink">↩</a></li>
            <li id="fn42">Hendrycks et al. 2020, op. cit. <a href="#ref42" class="backlink">↩</a></li>
            <li id="fn43">Estimated ceiling based on question validity studies. Gema, et al. "We Need to Talk about MMLU: The Importance of Studying Benchmark Errors." arXiv:2406.04127, 2024. <a href="https://arxiv.org/abs/2406.04127">https://arxiv.org/abs/2406.04127</a> A rigorous human baseline study on full MMLU would strengthen this estimate. <a href="#ref43" class="backlink">↩</a></li>
            <li id="fn44">Artificial Analysis, "Claude Opus 4.5 Benchmarks," November 2025. <a href="#ref44" class="backlink">↩</a></li>
            <li id="fn45">Various benchmark reports, December 2025. <a href="#ref45" class="backlink">↩</a></li>
            <li id="fn46">Gema et al. 2024, op. cit.; discussion in AI research community about MMLU saturation. <a href="#ref46" class="backlink">↩</a></li>
            <li id="fn47">Tool use is standard in frontier deployments. See Anthropic documentation, OpenAI function calling, etc. <a href="#ref47" class="backlink">↩</a></li>
            <li id="fn48">Hallucination in RAG systems is documented but rates vary. A systematic meta-analysis would strengthen this assessment. <a href="#ref48" class="backlink">↩</a></li>
            <li id="fn49">Systematic transfer learning benchmarks specifically designed for LLMs are lacking. <a href="#ref49" class="backlink">↩</a></li>
            <li id="fn50">Webb, Taylor, et al. "Emergent analogical reasoning in large language models." <em>Nature Human Behaviour</em> 7 (2023): 1526–1541. <a href="https://doi.org/10.1038/s41562-023-01659-w">https://doi.org/10.1038/s41562-023-01659-w</a> <a href="#ref50" class="backlink">↩</a></li>
            <li id="fn51">See various papers on LLM brittleness to surface feature changes; specific systematic examples would strengthen this claim. <a href="#ref51" class="backlink">↩</a></li>
            <li id="fn52">Long-context evaluation is an active research area. See RULER, SCROLLS, and related benchmarks. Systematic benchmarks for multi-source synthesis would strengthen this assessment. <a href="#ref52" class="backlink">↩</a></li>
            <li id="fn53">Belief revision in LLMs is under-studied. Systematic belief revision benchmarks would strengthen this assessment. <a href="#ref53" class="backlink">↩</a></li>
            <li id="fn54">Rein, David, et al. "GPQA: A Graduate-Level Google-Proof Q&A Benchmark." arXiv:2311.12022, 2023. <a href="https://arxiv.org/abs/2311.12022">https://arxiv.org/abs/2311.12022</a> <a href="#ref54" class="backlink">↩</a></li>
            <li id="fn55">GPQA paper reports ~65% expert validator accuracy. <a href="#ref55" class="backlink">↩</a></li>
            <li id="fn56">Artificial Analysis, op. cit. <a href="#ref56" class="backlink">↩</a></li>
            <li id="fn57">Various benchmark reports, December 2025. <a href="#ref57" class="backlink">↩</a></li>
            <li id="fn58">Ibid. <a href="#ref58" class="backlink">↩</a></li>
            <li id="fn59">AIME is the American Invitational Mathematics Examination; top 500 nationally typically requires ~90%+ score. <a href="#ref59" class="backlink">↩</a></li>
            <li id="fn60">OpenAI o3 announcement, December 2024. <a href="https://openai.com/index/deliberative-alignment/">https://openai.com/index/deliberative-alignment/</a> <a href="#ref60" class="backlink">↩</a></li>
            <li id="fn61">Frontier model AIME scores vary significantly. Official scores for Claude Opus 4.5 are not publicly available as of this writing. <a href="#ref61" class="backlink">↩</a></li>
            <li id="fn62">Causal reasoning benchmarks for LLMs include CRASS and various BIG-Bench tasks but lack standardization. Systematic causal reasoning benchmarks would strengthen this assessment. <a href="#ref62" class="backlink">↩</a></li>
            <li id="fn63">A systematic review of LLM causal reasoning capabilities would strengthen this assessment. <a href="#ref63" class="backlink">↩</a></li>
            <li id="fn64">Jimenez, Carlos E., et al. "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?" arXiv:2310.06770, 2023. <a href="https://arxiv.org/abs/2310.06770">https://arxiv.org/abs/2310.06770</a> <a href="#ref64" class="backlink">↩</a></li>
            <li id="fn65">Artificial Analysis, op. cit.; various reports cite ~81% for Claude Opus 4.5 on SWE-Bench Verified. <a href="#ref65" class="backlink">↩</a></li>
            <li id="fn66">Various benchmark reports. <a href="#ref66" class="backlink">↩</a></li>
            <li id="fn67">Ibid. <a href="#ref67" class="backlink">↩</a></li>
            <li id="fn68">Hallucination rates depend heavily on task type, domain, and evaluation methodology. Systematic meta-analyses are lacking. <a href="#ref68" class="backlink">↩</a></li>
            <li id="fn69">Current AI systems lack robotics integration for physical operations in most deployments. <a href="#ref69" class="backlink">↩</a></li>
            <li id="fn70">Noy, Shakked, and Whitney Zhang. "Experimental evidence on the productivity effects of generative artificial intelligence." <em>Science</em> 381.6654 (2023): 187–192. <a href="https://doi.org/10.1126/science.adh2586">https://doi.org/10.1126/science.adh2586</a> Other studies report varying results; systematic meta-analysis is lacking. <a href="#ref70" class="backlink">↩</a></li>
            <li id="fn71">Full task substitution (complete automation of job categories) remains limited as of late 2025. <a href="#ref71" class="backlink">↩</a></li>
            <li id="fn72">Mark Gubrud can be reached through public channels. We welcome his response to this analysis. <a href="#ref72" class="backlink">↩</a></li>
        </ol>
    </div>

    <p style="margin-top: 3em; font-size: 0.9em; color: var(--text-light); border-top: 1px solid var(--border); padding-top: 1.5em;">
        Document version 0.1 — December 25, 2025<br>
        © 2025 Dakota Schuck. Licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.
    </p>
</article>
